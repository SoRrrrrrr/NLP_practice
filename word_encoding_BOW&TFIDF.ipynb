{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization of Statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# BOW(Bag of Word)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['I love my dog.',\n",
    "             'I love my cat.',\n",
    "             'I love my dog and love my cat.',\n",
    "             'You love my dog!',\n",
    "             'Do you think my dog is amazing?']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Bag of Word(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>do</th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>think</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          amazing  and  cat  do  dog  is  love  my  think  you\n",
       "sentence                                                      \n",
       "0               0    0    0   0    1   0     1   1      0    0\n",
       "1               0    0    1   0    0   0     1   1      0    0\n",
       "2               0    1    1   0    1   0     2   2      0    0\n",
       "3               0    0    0   0    1   0     1   1      0    1\n",
       "4               1    0    0   1    1   1     0   1      1    1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Tokenization text data, 단어 빈도 수를 기반으로 하는 feature vector를 생성하는 데 사용됨\n",
    "count_vectorizer = CountVectorizer()\n",
    "## sentences data에 대한 피처 변환 수행 \n",
    "# fit_transform : 모델은 training data에 있는 평균, 분산을 학습 / 학습된 파라미터는 test data를 스케일하는 데 사용\n",
    "features = count_vectorizer.fit_transform(sentences)\n",
    "\n",
    "## features 객체를 NumPy 배열로 변환\n",
    "vectorized_sentences = features.toarray()\n",
    "## feature의 단어 list\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "## 벡터화된 문장과 피처 이름을 이용해 DataFrame 생성\n",
    "df = pd.DataFrame(vectorized_sentences, columns=feature_names)\n",
    "## dataframe의 index name 지정\n",
    "df.index.name='sentence'\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. TF-IDF(Term Frequency-Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amazing</th>\n",
       "      <th>and</th>\n",
       "      <th>cat</th>\n",
       "      <th>do</th>\n",
       "      <th>dog</th>\n",
       "      <th>is</th>\n",
       "      <th>love</th>\n",
       "      <th>my</th>\n",
       "      <th>think</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.606856</td>\n",
       "      <td>0.513275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.737922</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.515290</td>\n",
       "      <td>0.435829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.491109</td>\n",
       "      <td>0.396224</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.553364</td>\n",
       "      <td>0.468032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458054</td>\n",
       "      <td>0.387419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.655957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.247170</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209054</td>\n",
       "      <td>0.438724</td>\n",
       "      <td>0.353960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    amazing       and       cat        do       dog        is      love  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.606856  0.000000  0.606856   \n",
       "1  0.000000  0.000000  0.737922  0.000000  0.000000  0.000000  0.515290   \n",
       "2  0.000000  0.491109  0.396224  0.000000  0.276682  0.000000  0.553364   \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.458054  0.000000  0.458054   \n",
       "4  0.438724  0.000000  0.000000  0.438724  0.247170  0.438724  0.000000   \n",
       "\n",
       "         my     think       you  \n",
       "0  0.513275  0.000000  0.000000  \n",
       "1  0.435829  0.000000  0.000000  \n",
       "2  0.468032  0.000000  0.000000  \n",
       "3  0.387419  0.000000  0.655957  \n",
       "4  0.209054  0.438724  0.353960  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "## sentences 데이터에 대한 TF-IDF 기반 피처 변환 수행\n",
    "tfidf_sentences = tfidf_vectorizer.fit_transform(sentences)\n",
    "## TF-IDF feature 객체를 NumPy array로 변환\n",
    "tfidf_vect_sentences = tfidf_sentences.toarray()\n",
    "## TfidfVectorizer를 통해 추출한 feature name들을 가져옴\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "## 문서 상호간의 발생 빈도까지 감안이 된, 정규화된 TF-IDF Matrix\n",
    "# TF-IDF 벡터화된 문장과 피처 이름을 이용해 DataFrame 생성\n",
    "df = pd.DataFrame(tfidf_vect_sentences, columns=tfidf_feature_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Keras word encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keras API 이용\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'cat': 6, 'you': 7, 'and': 8, 'do': 9, 'think': 10, 'is': 11, 'amazing': 12}\n",
      "{1: '<OOV>', 2: 'my', 3: 'love', 4: 'dog', 5: 'i', 6: 'cat', 7: 'you', 8: 'and', 9: 'do', 10: 'think', 11: 'is', 12: 'amazing'}\n"
     ]
    }
   ],
   "source": [
    "# 문장으로부터 상위 100개 단어로 vocabulary 작성\n",
    "tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n",
    "## Word Index Vocabulary 작성\n",
    "# sentences에 포함된 문장들을 기반으로 단어의 토큰화를 수행, 각 단어에 고유한 인덱스를 할당\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "# 각 단어에 부여된 고유 인덱스 추출(양방향으로 확인)\n",
    "print(tokenizer.word_index)\n",
    "print(tokenizer.index_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text의 sentence 변환 및 padding\n",
    "<br>\n",
    "text_to_sentences : text list 내의 각 text를 수열로 convert\n",
    "- 입력 : text(strings) list\n",
    "- 반환 : sequence list\n",
    "<br>\n",
    "pad_sequences : 동일한 길이로 sequence를 Zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 3, 2, 4], [5, 3, 2, 6], [5, 3, 2, 4, 8, 3, 2, 6], [7, 3, 2, 4], [9, 7, 10, 2, 4, 11, 12]]\n",
      "\n",
      "[[ 5  3  2  4  0  0  0  0]\n",
      " [ 5  3  2  6  0  0  0  0]\n",
      " [ 5  3  2  4  8  3  2  6]\n",
      " [ 7  3  2  4  0  0  0  0]\n",
      " [ 9  7 10  2  4 11 12  0]]\n"
     ]
    }
   ],
   "source": [
    "## sentences 데이터를 sequence로 변환\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "## 시퀀스에 패딩 적용(문장의 뒤쪽을 패딩하고, 필요 시 뒤쪽을 잘라냄)\n",
    "padded = pad_sequences(sequences, padding='post',truncating='post')\n",
    "print(sequences)\n",
    "print()\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequenced sentence를 word sentence로 환원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i love my dog\n",
      "i love my cat\n",
      "i love my dog and love my cat\n",
      "you love my dog\n",
      "do you think my dog is amazing\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences:\n",
    "    sent=[]\n",
    "    for idx in seq:\n",
    "        sent.append(tokenizer.index_word[idx])\n",
    "    print(' '.join(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot Encoding 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(padded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

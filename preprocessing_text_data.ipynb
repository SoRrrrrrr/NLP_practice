{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### text data 전처리\n",
    "\n",
    "- nltk를 이용하여 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenize(형태소 분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'is',\n",
       " 'a',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'for',\n",
       " 'building',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'work',\n",
       " 'with',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'It',\n",
       " 'provides',\n",
       " 'easy-to-use',\n",
       " 'interfaces',\n",
       " 'to',\n",
       " 'overs',\n",
       " '50',\n",
       " 'corpora',\n",
       " 'and',\n",
       " 'lexical',\n",
       " 'resources',\n",
       " 'such',\n",
       " 'as',\n",
       " 'WordNet',\n",
       " ',',\n",
       " 'along',\n",
       " 'with',\n",
       " 'a',\n",
       " 'suite',\n",
       " 'of',\n",
       " 'text',\n",
       " 'processing',\n",
       " 'libraries',\n",
       " 'for',\n",
       " 'clasification',\n",
       " ',',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'tagging',\n",
       " ',',\n",
       " 'parsing',\n",
       " ',',\n",
       " 'and',\n",
       " 'semantic',\n",
       " 'reasoning',\n",
       " ',',\n",
       " 'wrappers',\n",
       " 'for',\n",
       " 'industrial-strength',\n",
       " 'NLP',\n",
       " 'libraries',\n",
       " ',',\n",
       " 'and',\n",
       " 'an',\n",
       " 'active',\n",
       " 'discussion',\n",
       " 'forum',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전처리 하고자 하는 문장 \n",
    "sentence = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to overs 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for clasification, tokenization, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'\n",
    "\n",
    "# 각 문장 tokenize\n",
    "nltk.word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. POS tagging(품사 태깅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('leading', 'VBG'),\n",
       " ('platform', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('building', 'VBG'),\n",
       " ('Python', 'NNP'),\n",
       " ('programs', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('provides', 'VBZ'),\n",
       " ('easy-to-use', 'JJ'),\n",
       " ('interfaces', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('overs', 'NNS'),\n",
       " ('50', 'CD'),\n",
       " ('corpora', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('lexical', 'JJ'),\n",
       " ('resources', 'NNS'),\n",
       " ('such', 'JJ'),\n",
       " ('as', 'IN'),\n",
       " ('WordNet', 'NNP'),\n",
       " (',', ','),\n",
       " ('along', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('suite', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('text', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('libraries', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('clasification', 'NN'),\n",
       " (',', ','),\n",
       " ('tokenization', 'NN'),\n",
       " (',', ','),\n",
       " ('tagging', 'VBG'),\n",
       " (',', ','),\n",
       " ('parsing', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('semantic', 'JJ'),\n",
       " ('reasoning', 'NN'),\n",
       " (',', ','),\n",
       " ('wrappers', 'NNS'),\n",
       " ('for', 'IN'),\n",
       " ('industrial-strength', 'JJ'),\n",
       " ('NLP', 'NNP'),\n",
       " ('libraries', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('an', 'DT'),\n",
       " ('active', 'JJ'),\n",
       " ('discussion', 'NN'),\n",
       " ('forum', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = nltk.word_tokenize(sentence)\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  eliminate Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'bengali',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179 \n",
      "\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# 영어의 stopwords list를 불러와 stopWords 변수에 저장함\n",
    "# stopwords는 다 소문자로 구성되어 있음\n",
    "stopWords=stopwords.words('english')\n",
    "\n",
    "print(len(stopWords),'\\n')\n",
    "print(stopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.', 'It', 'provides', 'easy-to-use', 'interfaces', 'to', 'overs', '50', 'corpora', 'and', 'lexical', 'resources', 'such', 'as', 'WordNet', ',', 'along', 'with', 'a', 'suite', 'of', 'text', 'processing', 'libraries', 'for', 'clasification', ',', 'tokenization', ',', 'tagging', ',', 'parsing', ',', 'and', 'semantic', 'reasoning', ',', 'wrappers', 'for', 'industrial-strength', 'NLP', 'libraries', ',', 'and', 'an', 'active', 'discussion', 'forum', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.', 'provides', 'easy-to-use', 'interfaces', 'overs', '50', 'corpora', 'lexical', 'resources', 'WordNet', ',', 'along', 'suite', 'text', 'processing', 'libraries', 'clasification', ',', 'tokenization', ',', 'tagging', ',', 'parsing', ',', 'semantic', 'reasoning', ',', 'wrappers', 'industrial-strength', 'NLP', 'libraries', ',', 'active', 'discussion', 'forum', '.']\n"
     ]
    }
   ],
   "source": [
    "# 문장에서 stopwords 제거\n",
    "result = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token.lower() not in stopWords:\n",
    "        result.append(token)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', 'provides', 'easy-to-use', 'interfaces', 'overs', '50', 'corpora', 'lexical', 'resources', 'WordNet', 'along', 'suite', 'text', 'processing', 'libraries', 'clasification', 'tokenization', 'tagging', 'parsing', 'semantic', 'reasoning', 'wrappers', 'industrial-strength', 'NLP', 'libraries', 'active', 'discussion', 'forum']\n"
     ]
    }
   ],
   "source": [
    "# 쉼표, 마침표도 stopwords에 추가하여 다시 적용\n",
    "stopWords = stopwords.words('english')\n",
    "stopWords.append(',')\n",
    "stopWords.append('.')\n",
    "\n",
    "result = []\n",
    "\n",
    "for token in tokens:\n",
    "    if token.lower() not in stopWords:\n",
    "        result.append(token)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 영화 리뷰 데이터 전처리 *Lemmatizing*\n",
    "Lemmatization ; 단어의 형태소적 & 사전적 분석을 통해 파생적 의미를 제거하고, 어근에 기반하여 *기본 사전형인 lemma*를 찾는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordNetLemmatizer 객체 생성\n",
    "lemmatizer = nltk.wordnet.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "# WordNetLemmatize는 더 정확한 분석을 위해 PoS 정보를 추가로 입력받을 수 있음\n",
    "# n ; 명사(default), a ; 형용사, v ; 동사, r ; 부사\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "\n",
      "ran\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "print(lemmatizer.lemmatize(\"better\",pos='a'))\n",
    "print()\n",
    "print(lemmatizer.lemmatize(\"ran\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords 제거 작업에 pos tagging 작업 추가\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(',')\n",
    "stop_words.append('.')\n",
    "\n",
    "file = open('moviereview.txt','r',encoding='utf-8')\n",
    "lines = file.readlines()\n",
    "\n",
    "sentences = lines[1]\n",
    "# 단어 기준으로 tokenize\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "\n",
    "#for문을 통해 stopwords제거 & lemmatization\n",
    "lemmas = [] # lemmatized 된 결과\n",
    "\n",
    "for token, pos in tagged_tokens:\n",
    "    if token.lower() not in stop_words:\n",
    "        if pos.startwith('N'):\n",
    "            lemmas.append(lemmatizer.lemmatize(token, pos='n'))\n",
    "        elif pos.startwith('J'):\n",
    "            lemmas.append(lemmatizer.lemmatize(token, pos='a'))\n",
    "        if pos.startwith('V'):\n",
    "            lemmas.append(lemmatizer.lemmatize(token, pos='v'))\n",
    "        else:\n",
    "            lemmas.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "print('Lemmas of : '+sentence)\n",
    "print(lemmas)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
